import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler

# Data
X = np.array([150, 180, 164, 162, 181, 182, 173, 190, 171, 170, 181, 182, 189, 184, 209, 210])
y = np.array([51, 52, 54, 53, 55, 59, 61, 59, 63, 76, 64, 66, 69, 72, 70, 80])

# Feature Scaling
scaler_X = StandardScaler()
scaler_y = StandardScaler()

X_scaled = scaler_X.fit_transform(X.reshape(-1, 1)).flatten()
y_scaled = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()

# Initialize theta parameters and learning rate
theta0 = 0
theta1 = 0
learning_rate = 1e-3  # Tăng learning rate
iterations = 30000  # Tăng số vòng lặp
cost_history = []

def predict(X, theta0, theta1):
    return theta0 + theta1 * X

def cost_function(X, y, theta0, theta1):
    m = len(X)
    return (1 / (2 * m)) * np.sum((predict(X, theta0, theta1) - y) ** 2)

def gradient_descent(X, y, theta0, theta1, learning_rate):
    m = len(X)
    y_pred = predict(X, theta0, theta1)
    gradient0 = (1 / m) * np.sum(y_pred - y)
    gradient1 = (1 / m) * np.sum((y_pred - y) * X)
    new_theta0 = theta0 - learning_rate * gradient0
    new_theta1 = theta1 - learning_rate * gradient1
    return new_theta0, new_theta1

# Perform gradient descent
for _ in range(iterations):
    theta0, theta1 = gradient_descent(X_scaled, y_scaled, theta0, theta1, learning_rate)
    cost_history.append(cost_function(X_scaled, y_scaled, theta0, theta1))

print("theta0:", theta0, "theta1:", theta1)

